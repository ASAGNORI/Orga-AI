"""
M√≥dulo para gerenciar o contexto das mensagens de chat.
Implementa estrat√©gias de limita√ß√£o e resumo de contexto.
"""
import logging
from typing import List, Dict, Any, Tuple

logger = logging.getLogger(__name__)

class ContextManager:
    """
    Gerencia o contexto das mensagens enviadas para o LLM.
    Implementa estrat√©gias para otimizar o desempenho:
    1. Limita√ß√£o do contexto (n√∫mero m√°ximo de mensagens)
    2. Resumo de conversas longas
    """
    
    def __init__(self, max_messages: int = 0):
        """
        Inicializa o gerenciador de contexto.
        
        Args:
            max_messages: N√∫mero m√°ximo de pares mensagem/resposta a incluir no contexto
                          (0 para modo turbo sem contexto)
        """
        self.max_messages = max_messages  # 0 = modo turbo (sem hist√≥rico)
    
    def optimize_context(self, history: List[Tuple[str, str]]) -> List[Dict[str, str]]:
        """
        Otimiza o contexto da conversa para envio ao LLM.
        Modo turbo (max_messages=0): retorna lista vazia para m√°xima performance.
        
        Args:
            history: Lista de tuplas (mensagem_usuario, resposta_ai)
            
        Returns:
            Lista formatada de mensagens para o LLM, com contexto otimizado
        """
        # Para desempenho m√°ximo (modo turbo), n√£o usar contexto
        if self.max_messages == 0 or not history:
            logger.info("üöÄ Modo TURBO ativado: Ignorando todo hist√≥rico para m√°xima performance")
            return []
            
        optimized_context = []
        
        # Limitar o n√∫mero de mensagens no hist√≥rico
        limited_history = history[:self.max_messages] if history else []
        
        # Converter para o formato esperado pelo LLM
        # Apenas √∫ltimo par de mensagens para m√°xima performance
        for user_msg, ai_msg in limited_history:
            # Limitar tamanho de cada mensagem para economizar tokens
            user_msg_short = user_msg[:150] + "..." if len(user_msg) > 150 else user_msg
            ai_msg_short = ai_msg[:150] + "..." if len(ai_msg) > 150 else ai_msg
            
            optimized_context.append({"role": "user", "content": user_msg_short})
            optimized_context.append({"role": "assistant", "content": ai_msg_short})
        
        logger.info(f"Contexto otimizado: {len(limited_history)} de {len(history)} mensagens inclu√≠das (truncadas para m√°x. 150 caracteres)")
        return optimized_context
    
    def summarize_history(self, history: List[Tuple[str, str]]) -> str:
        """
        Cria um resumo do hist√≥rico da conversa quando √© muito longo.
        Usado para conversas extensas onde enviar todo o hist√≥rico seria ineficiente.
        
        Args:
            history: Lista de tuplas (mensagem_usuario, resposta_ai)
            
        Returns:
            Resumo do hist√≥rico da conversa
        """
        # Implementa√ß√£o b√°sica por enquanto - apenas menciona quantas mensagens foram omitidas
        if len(history) <= self.max_messages:
            return ""
            
        omitted_count = len(history) - self.max_messages
        return f"[Contexto: Esta conversa tem {len(history)} mensagens anteriores, mas {omitted_count} foram omitidas para otimiza√ß√£o.]"

# Inst√¢ncia global para uso em toda a aplica√ß√£o
context_manager = ContextManager(max_messages=5)  # Mant√©m as √∫ltimas 5 mensagens para melhor contexto
 